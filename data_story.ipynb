{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127dfae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cd1de",
   "metadata": {},
   "source": [
    "We start by loading the previously created dataframe with the answers to the questions we asked gpt. We also read the 20 questions, which are stored in `questions` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb9b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "romance_with_binary = pd.read_csv('./Data/Preprocessed/romances.with.binary.tsv', delimiter='\\t')\n",
    "romance_with_binary.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae074fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/trope_questions.txt', 'r')\n",
    "\n",
    "line = f.readline()\n",
    "questions = []\n",
    "\n",
    "while len(line)!=0:\n",
    "    questions.append(line.strip())\n",
    "    line = f.readline()\n",
    "    \n",
    "questions = questions[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91138309",
   "metadata": {},
   "source": [
    "# A) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5935d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_answers = romance_with_binary[['movie_id', 'binary_answers']].copy()\n",
    "gpt_answers['response_length'] = gpt_answers['binary_answers'].apply(lambda ans: len(ans))\n",
    "#gpt_answers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeca9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bits = {'0', '1', '2'}\n",
    "gpt_answers['binary_answers'] = gpt_answers['binary_answers'].apply(lambda b: ''.join(c for c in b if c in bits))\n",
    "gpt_answers['response_length'] = gpt_answers['binary_answers'].apply(lambda ans: len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd0997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_answers['binary_answers'] = gpt_answers['binary_answers'].apply(lambda b: b if len(b)==20 else '2'*20)\n",
    "gpt_answers.drop(columns=['response_length'], inplace=True)\n",
    "gpt_answers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ccd3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for q_nb, question in enumerate(questions):\n",
    "    gpt_answers[f'q_{q_nb}'] = gpt_answers['binary_answers'].apply(lambda b: int(b[q_nb]))\n",
    "    \n",
    "gpt_answers.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9ec74",
   "metadata": {},
   "source": [
    "And let's plot how many answers to the questions were negative, positive and unknown for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    fig, ax = plt.subplots(4, 5, figsize= (20,8), sharey = True, sharex = True)\n",
    "\n",
    "    for i in range(20):\n",
    "        sbplt = ax[i%4, math.floor(i/4)]\n",
    "        sbplt.hist(df[f'q_{i}'])\n",
    "        sbplt.set_title(f'Question {i}')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Histogram of answers (0, 1, and 2) for each question', y=1.05, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "#plot(gpt_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_charts(df):\n",
    "    fig, ax = plt.subplots(nrows = 4, ncols = 5, figsize = (12,10))\n",
    "\n",
    "    for r in range(4):\n",
    "        for c in range(5):\n",
    "            ax[r,c].pie(df[f'q_{r*5+c}'].value_counts(),\n",
    "                        labels = df[f'q_{r*5+c}'].dropna().unique(),\n",
    "                        autopct=lambda p: '{:.1f}%'.format(p),\n",
    "                        startangle=90, shadow=False)\n",
    "            ax[r,c].set_title(f'Question {r*5+c}')\n",
    "    \n",
    "    fig.suptitle('Pie charts of percentage of answers (0, 1, and 2) for each question', y=1.01, fontsize=15)\n",
    "    fig.tight_layout()\n",
    "            \n",
    "#plot_pie_charts(gpt_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c7ab6-9485-4121-ba8a-b270c7d71f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relevant_answers = gpt_answers[gpt_answers['binary_answers']!='2'*20].copy()\n",
    "\n",
    "print(f\"There are {relevant_answers['binary_answers'].count()-relevant_answers['binary_answers'].str.contains('1').sum()} movies that do not even contain a 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7a8df-f8b1-44fc-9450-b56ab4cb8bcc",
   "metadata": {},
   "source": [
    "As seen above, there are around 600 movies that have no positive answer at all. Some examples of these movies are Spirit, Harry Potter movies... Note that these movies should not be considered romance movies in the first place ! It is thus good that GPT outputs a `00000000000000000000` when analyzing them. \n",
    "\n",
    "In fact, this gives us a rationale to trim even more the dataset : if a movie does not contain any of the 20 clichés, is it in fact a romance movie ? It seems that most of the all `0`'s strings are in fact from movies that are not romance centered at all. We thus choose an (arbitrary) threshold to consider a movie `Romance` : it should contain at least 3 positive answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First remove any string consisting of only 2s\n",
    "#df = gpt_answers[gpt_answers['binary_answers']!='2'*20].copy()\n",
    "\n",
    "# Define the questions list, ie all columns\n",
    "questions_list = ['q_0', 'q_1', 'q_2', 'q_3', 'q_4', 'q_5', 'q_6', 'q_7', 'q_8', 'q_9', 'q_10', 'q_11', 'q_12', 'q_13', 'q_14', 'q_15', 'q_16', 'q_17', 'q_18', 'q_19']\n",
    "\n",
    "# We keep only the strings with at least a 1. \n",
    "# This way we remove the all 0s strings, the all 2s strings, and the mixed strings.\n",
    "data = gpt_answers[gpt_answers['binary_answers'].str.contains('1')].copy()\n",
    "\n",
    "filenames = data['movie_id']\n",
    "filenames = filenames.apply(lambda x : str(x) + '.xml.gz').values\n",
    "\n",
    "# 4380 different binary answers\n",
    "print(f\"There are {len(gpt_answers['binary_answers'].unique())} different answers.\")\n",
    "\n",
    "# Remove all the entries that (somehow) have more than 20 positive answers, and then plot the histogram of #1s.\n",
    "data['total'] = data[questions_list].sum(axis = 1)\n",
    "data = data[21 > data['total']]\n",
    "plt.hist(data['total'], bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25786e83-0318-4467-baa5-ef520e2a09f2",
   "metadata": {},
   "source": [
    "We should expect, after keeping only movies with 3 positive answers or more, to have around 4500 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc971f-3c0d-450d-9c32-0c6b4d3c590e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep only entries with at least 3 positive answers: this captures the \"real\" romance movies.\n",
    "data = data[data['total'] > 0]\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c6da7-4f7b-48aa-8467-b20774691787",
   "metadata": {},
   "source": [
    "Now that we refined our dataset, we only have $4653$ movies now. Let's plot each question's statistics !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcda8c-9241-4915-abe8-ec2d023afb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_pie_charts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83134297",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_keep = ['movie_id', 'movie_release', 'binary_answers']\n",
    "for i in range(len(questions)):\n",
    "    col_to_keep.append(f'q_{i}')\n",
    "    \n",
    "# Perform a right merge : we only care about the relevant answers\n",
    "gpt_year = romance_with_binary.merge(data, on=['movie_id', 'binary_answers'], how='right')[col_to_keep]\n",
    "gpt_year.replace(2, np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229bc0fe-4aa6-4260-89f1-a6aedd454e46",
   "metadata": {},
   "source": [
    "At this point, we can wonder : is there a clear trend as to how clichés evolve over time ? As an example, could we say that modern movies contain less weddings stopped at the altar ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20723f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_answer_by_year(df): \n",
    "    fig, ax = plt.subplots(4, 5, figsize= (20,8), sharey = True, sharex = True)\n",
    "\n",
    "    for i in range(20):\n",
    "        ax[i%4, math.floor(i/4)].set_title(f'Question {i}')\n",
    "        sns.lineplot(df, x='movie_release', y=f'q_{i}', ax=ax[i%4, math.floor(i/4)])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "#plot_answer_by_year(gpt_year)\n",
    "#plot_answer_by_year(gpt_year[gpt_year['movie_release']>1920])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_answer_by_year(gpt_year[gpt_year['movie_release']>1920])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b989c-cc8b-436d-9a99-cae0a3f04a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# B) Analysis of single questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b165b-e25b-4ee2-aea2-0c89093cf4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## When sophie gives tsv : just import it as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935198f3-6e86-41c8-9790-b6347e26204e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trimmed = data.drop(['binary_answers'], axis = 1)\n",
    "trimmed = trimmed.replace(2, np.NaN)\n",
    "#trimmed = trimmed.astype(pd.Int64Dtype())\n",
    "df = trimmed.merge(romance_with_binary, on = 'movie_id', how = 'left')\n",
    "\n",
    "df.drop_duplicates('movie_id', inplace=True)\n",
    "\n",
    "# TODO : MAYBE REMOVE THIS !\n",
    "#df = df[df.nb_votes > 10000]\n",
    "\n",
    "#print(df.info())\n",
    "df = df.sort_values(by='nb_votes')\n",
    "plt.plot(np.log(df.nb_votes), df.rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f335b-05b4-4f4f-8efa-20f8d3fca525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mva = df.dropna(subset = ['rating', 'nb_votes'])\n",
    "df_mva['mva'] = df_mva['rating'].rolling(100).mean()\n",
    "df_mva.dropna(subset = 'mva')\n",
    "df_mva = df_mva.sort_values(by='nb_votes')\n",
    "# moving average with a window size of 100\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(np.log(df_mva.nb_votes), df_mva.mva)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dce7a1-064f-461d-96c0-dd88a6be2fc6",
   "metadata": {},
   "source": [
    "We want to reduce this bias! Hence, we only keep movies with more than 500 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5083e4-4961-41a8-8562-76f5adbaa637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[(df.nb_votes > 500)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11675bc8-6325-4fda-ac55-da8362ea8fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x='movie_release', y='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98bd8a-9da7-4c5a-8b98-6260381e56b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"ratings_with_weight\"] = [ v + w*1j for v,w in zip(df.rating, df.nb_votes)]\n",
    "def weighted_mean(x, **kws):\n",
    "    return np.sum(np.real(x) * np.imag(x)) / np.sum(np.imag(x))\n",
    "\n",
    "def plot_error_bars(df, nr_rows, nr_cols, weights):\n",
    "    \n",
    "    fig, ax = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3), squeeze=False)\n",
    "    \n",
    "    for r in range(nr_rows):\n",
    "        for c in range(nr_cols):\n",
    "            \n",
    "            index = r * 5 + c\n",
    "            question = 'q_' + str(index)\n",
    "            \n",
    "            if weights : \n",
    "                sns.barplot(data=df, x= question, y='ratings_with_weight', ax = ax[r][c], estimator = weighted_mean, errorbar=('ci', 95))\n",
    "                ax[r][c].set_ylim(6.65,7.5)\n",
    "            else :\n",
    "                sns.barplot(data=df, x= question, y='rating', ax = ax[r][c])\n",
    "                ax[r][c].set_ylim(6.2,6.9)\n",
    "\n",
    "            ax[r][c].set_ylabel('average rating')\n",
    "            ax[r][c].set_title(question)\n",
    "            #ax[r,c].set_title(question)\n",
    "    \n",
    "    fig.suptitle('AHOUUUUUUUUUUUUU', y=1.01, fontsize=15)\n",
    "    plt.tight_layout()    \n",
    "    plt.show()\n",
    "    \n",
    "plot_error_bars(df, 4, 5, False)\n",
    "#plot_error_bars(df, 4, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49120d50-22b8-49ce-8d3d-cdbb8afa20f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This functions is to be used when analyzing pairs of questions. \n",
    "def pair_analysis(id1, id2) : \n",
    "\n",
    "    id1 = str(id1)\n",
    "    id2 = str(id2)\n",
    "    q1 = 'q_' + id1\n",
    "    q2 = 'q_' + id2 \n",
    "    \n",
    "    df_cleaned = df.dropna(subset=[q1, q2])\n",
    "    dft = df_cleaned.copy()\n",
    "    dft['merged_' + id1 + id2] = list(zip(df_cleaned[q1], df_cleaned[q2]))\n",
    "    \n",
    "    sns.barplot(data = dft, x='merged_'+ id1 + id2,  y='rating')\n",
    "\n",
    "pair_analysis(7, 15)\n",
    "plt.ylim(6.1, 6.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d08d0-68c5-4b2c-9340-25cfcfff7831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data = df, x= 'movie_release', y='rating', hue = 'q_0', palette = sns.color_palette()[:df.q_0.nunique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9e472-89ed-40cc-8cd2-28e7b791caf6",
   "metadata": {},
   "source": [
    "# C) Analysis of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d08f83-6746-49bc-b9d2-e1ebd633fab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=questions_list)\n",
    "\n",
    "#define predictor and response variables\n",
    "X, y = df_cleaned[questions_list], df_cleaned.rating\n",
    "\n",
    "#fit regression model\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "#print(reg.coef_)\n",
    "print(f\"For this regression, r-squared score is : {reg.score(X, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02daeb-75e0-4d1c-8f28-d63b4703679b",
   "metadata": {},
   "source": [
    "The model may be too simple... We should maybe try to take into account interaction terms ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd8f2f-9df6-44da-8003-d5a1ccd82b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = df_cleaned[questions_list].to_numpy(), df_cleaned.rating.to_numpy()\n",
    "\n",
    "print(f\"Initially: \\nX has shape {X.shape}\\ny has shape {y.shape}\\n\")\n",
    "\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(interaction_only=True,include_bias = False, degree = degree)\n",
    "X_interactions = poly.fit_transform(X)\n",
    "print(f\"After adding interactions terms up to degree {degree}: \\nX has shape : {X_interactions.shape}\\n\")\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_interactions,y)\n",
    "print(f\"The r-squared score is {reg.score(X_interactions, y)}\")\n",
    "\n",
    "cv_scores = cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Cross-Validation mean-squared error : {-cv_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aba55-4530-43c9-bbdb-e74224c32569",
   "metadata": {},
   "source": [
    "This is bad. Whether it's degree 2,3 or 4, we either overfit or underfit... At this point we can wonder whether it's infact a problem of our model, or of the data. Let us try one last model : the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7faf9-8eb3-45e8-b3c5-07b5b34d4c8f",
   "metadata": {},
   "source": [
    "# D) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7cdf1-d53f-4dc2-8fcb-e533398340a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove questions 14, 15 as they were observed to be rather unaccurate.\n",
    "cleaned_questions_list = ['q_0', 'q_1', 'q_2', 'q_3', 'q_4', 'q_5', 'q_6', 'q_7', 'q_8', 'q_9', 'q_10', 'q_11', 'q_12', 'q_13', 'q_16', 'q_17', 'q_18', 'q_19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66012601-bf17-480d-8a8d-32a7480a5f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=cleaned_questions_list)\n",
    "X, y = df_cleaned[cleaned_questions_list], df_cleaned.rating\n",
    "\n",
    "# Create an instance of a random forest regressor, using sklearn.\n",
    "regressor = RandomForestRegressor(n_estimators=5, random_state=42, max_depth = 15)\n",
    "\n",
    "# Fit the regressor with X and y data.\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Make predictions for the initial data.\n",
    "predictions = regressor.predict(X)\n",
    " \n",
    "# Extract MSE and R2 scores to evaluate the model.\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    " \n",
    "r2 = r2_score(y, predictions)\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1879d67-d26b-4fd0-802d-9c70221e04da",
   "metadata": {},
   "source": [
    "This seems okay, let us perform cross-validation to verify that we indeed predict accurately !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3ff91-8a50-494f-9afc-c8dc9ba88170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(regressor, X, y, cv = 5, scoring = 'neg_mean_squared_error')  # Adjust the number of folds as needed\n",
    "\n",
    "print(f'Cross-Validation scores: {cv_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ad701-fec2-4664-8069-d592996472cc",
   "metadata": {},
   "source": [
    "We obtain a MSE of around 1 in each case, but the R2 was high. This means we are (again) overfitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c575f8-209e-4563-896a-b2bec4407b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances = regressor.feature_importances_\n",
    "print('Feature Importances:', feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307eeafa-b65b-4621-a26c-7e81e57366ba",
   "metadata": {},
   "source": [
    "Whether it is a random forest, a naive regression or a regression with interaction terms, we always do bad in the prediction. \n",
    "\n",
    "We could thus say there is no clear and simple model as to how movies are rated. In a sense, this is reassuring : humans are not robots, and we do not appreciate movies based solely on a yes-no flow chart concerning the plot ! There is much more to movies than just the plots :)\n",
    "\n",
    "Also, keep in mind that although checked to be rather correct, GPT (and any question-answering model nowadays) results may be inaccurate, especially when a question expresses \"feelings\", or requires interpretation (e.g. \"Is the ending sad\").\n",
    "\n",
    "In the end, does this mean we can not do anything using this data ? Not at all ! In fact, although there is no clear way to predict a movie's success based on its tropes, we can still analyze which tropes are positively and negatively correlated (think about the barplots above). We can even do statistical tests based on matchings, on a subset of relevant questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a132a4-f253-4aa2-ac47-20ff33adf17e",
   "metadata": {},
   "source": [
    "# E) Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d69d7-62cc-4e67-ae7e-a365db11fab1",
   "metadata": {},
   "source": [
    "As seen above, many questions seem to be irrelevant. We thus focus on 6 or 7 questions, in order to perform matchings and statistical tests. That way, we ensure that we have enough movies to make a matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267b3ad-a9b5-49bd-baef-e12996c08466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import  LinearRegression\n",
    "\n",
    "model =  LinearRegression()  # Choose an appropriate model\n",
    "rfe = RFECV(model, step=1, cv=5, scoring='neg_mean_squared_error')\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "selected_features = list(zip(X.columns[fit.support_], fit.estimator_.coef_)) \n",
    "score = fit.score(X, y)\n",
    "\n",
    "selected_features.sort(key = lambda x: np.abs(x[1]))\n",
    "print(\"Selected features/Feature importance :\")\n",
    "for feature in selected_features : \n",
    "    print(f\"   {feature[0]} : {round(feature[1], 2)}\")\n",
    "    \n",
    "print(f\"Score : {round(score, 2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410ed7c-c334-4ea4-9b2d-4c45ccf04e5c",
   "metadata": {},
   "source": [
    "Note that again, the score is really low. This is normal, as, as previously seen, there is no way to predict accurately ratings of the movies.\n",
    "\n",
    "There are 9 significant features, which yields 512 bins. This is a bit too much for 4000 movies, so we choose the 7 most important questions, ie only 128 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23850788-55b9-439b-bd16-06096bcb7c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_questions = ['q_0', 'q_3', 'q_4', 'q_18']\n",
    "\n",
    "# Questions to remove, computes 'questions_list - selected_questions'\n",
    "questions_to_remove = [x for x in questions_list if x not in selected_questions]\n",
    "\n",
    "print(questions_to_remove)\n",
    "\n",
    "filtered_df = df_cleaned.drop(columns = questions_to_remove)\n",
    "\n",
    "filtered_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42e3f9-058b-4d68-b1b9-7ca83a6b18ee",
   "metadata": {},
   "source": [
    "# F) Observational Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc625c-1010-4571-8372-7882cb59c2b5",
   "metadata": {},
   "source": [
    "We were unable to predict ratings of movies based on their ratings, and also unable to perform a clean regression. But this does not mean we are done ! We can still analyze whether clichés are positively or negatively correlated ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a9466-3715-4008-a32f-66be94efe56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching = filtered_df.groupby(selected_questions).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02fa97-7639-4c8c-8ede-e8e10c56ea28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_n_indices(data, n):\n",
    "    ind = np.argpartition(data, -n)[-n:]\n",
    "    ind = ind[np.argsort(data[ind])][::-1]\n",
    "    return ind\n",
    "\n",
    "print(matching.iloc[top_n_indices(matching['count'], 8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abe282-6445-4d66-8a29-58b3abf79ce9",
   "metadata": {},
   "source": [
    "### 1) Question 0 : do people hate weddings stopped at the altar ? :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248296f-2dcd-47eb-959d-7bb69467f4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.barplot(filtered_df[['q_0', 'rating', 'nb_votes']], x='q_0', y ='nb_votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb987b3-a47c-48fa-b05a-e524aa1f25ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_matching(questions, filtered_data):\n",
    "\n",
    "    # Questions should be of form \"1000\", \"0100\", \"0010\", or \"0001\".\n",
    "    # This is because we noticed that in any case, the biggest control group is the \"0000\" one\n",
    "    split = list(questions)    \n",
    "    q_0 = int(split[0])\n",
    "    q_3 = int(split[1])\n",
    "    q_4 = int(split[2])\n",
    "    q_18 = int(split[3])\n",
    "        \n",
    "    # First, we extract the indices of all the movies with binary answers:\n",
    "    # Form \"0000\" for the control group\n",
    "    # Form \"0100\" for the treatment group (e.g.)\n",
    "    control_indices = filtered_df[(filtered_df['q_0'] == 0) & (filtered_df['q_3'] == 0) & (filtered_df['q_4'] == 0) & (filtered_df['q_18'] == 0)].index\n",
    "    treatment_indices = filtered_df[(filtered_df['q_0'] == q_0) & (filtered_df['q_3'] == q_3) & (filtered_df['q_4'] == q_4) & (filtered_df['q_18'] == q_18)].index\n",
    "    total = list(control_indices) + list(treatment_indices)\n",
    "    \n",
    "    # We now extract the sub-dataframes from the indices.\n",
    "    control = filtered_df.loc[control_indices]\n",
    "    control = control.sort_values(by = 'nb_votes')\n",
    "\n",
    "    treatment = filtered_df.loc[treatment_indices]\n",
    "    treatment = treatment.sort_values(by = 'nb_votes')\n",
    "    \n",
    "    # This creates the columns names, useful for later.\n",
    "    cols_x = list(treatment.columns + '_x')\n",
    "    cols_x.remove('nb_votes_x')\n",
    "    cols_y = list(treatment.columns + '_y')\n",
    "    cols_y.remove('nb_votes_y')\n",
    "\n",
    "    # Perform a merge : for each movie in the treatment group, this pairs it\n",
    "    # with the movie in the control group with the closest number of votes    \n",
    "    test = pd.merge_asof(treatment, control, on=\"nb_votes\", direction=\"nearest\")\n",
    "\n",
    "    # After the pairing, extract the groups into two separate datasets\n",
    "    treatment_matched = test[cols_x].copy()\n",
    "    treatment_matched.rename(mapper = lambda name : name[:-2], axis='columns', inplace=True)\n",
    "    treatment_matched = treatment_matched.merge(filtered_df[['movie_id','nb_votes']],on='movie_id', how='left')\n",
    "\n",
    "    control_matched = test[cols_y].copy()\n",
    "    control_matched.rename(mapper = lambda name : name[:-2], axis='columns', inplace=True)\n",
    "    control_matched = control_matched.merge(filtered_df[['movie_id','nb_votes']],on='movie_id', how='left')\n",
    "    \n",
    "\n",
    "    matching = pd.concat((control_matched, treatment_matched))\n",
    "    \n",
    "    print(f\"There are {len(control_matched)} control movies, and {len(treatment_matched)} treatment movies\")\n",
    "    #control_matched.drop_duplicates(subset='movie_id', keep='first', inplace = True)\n",
    "    return matching, control_matched, treatment_matched\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a86dbf-5224-419f-b752-c918c60c5417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_dict = {'q_0' : '1000', 'q_3' :'0100', 'q_4' :'0010', 'q_18' : '0001'}\n",
    "\n",
    "def get_standardized_mean_diff(treatment, control):\n",
    "    return np.abs(treatment.mean()-control.mean())/np.sqrt(treatment.std()**2+control.std()**2)\n",
    "\n",
    "\n",
    "def observational_regression(question) :\n",
    "    \n",
    "    matching, control_matched, treatment_matched = create_matching(features_dict[question], filtered_df)\n",
    "    smd = get_standardized_mean_diff(control_matched['nb_votes'], treatment_matched['nb_votes'])\n",
    "    print(f\"SMD = {round(smd, 4)}\")\n",
    "\n",
    "    mod = smf.ols(formula='rating ~ ' + question, data=matching)\n",
    "    print(mod.fit().summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e4589-a309-4e71-9765-ddd68f76c31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observational_regression('q_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ac596-32ad-4954-87e9-625fa539b5af",
   "metadata": {
    "tags": []
   },
   "source": [
    "That's good ! We found 576 pairs of movies that have the same answers on the most important questions. Moreover, we made an approximate matching based on their number of votes. This is confirmed through the SMD criterion, which is extremely low (around .0002).\n",
    "\n",
    "This way, we reduces all the possible biases. In the end, we can confirm : as the p-value is really low, we can thus accept the hypothesis that average rating for movies with a wedding stopped at the altar and without is the same!\n",
    "\n",
    "Conclusion : weddings stopped at the altar are BAD !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
